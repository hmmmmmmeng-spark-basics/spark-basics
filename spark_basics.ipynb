{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = '/usr/share/spark'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/share/miniconda2/envs/py36/bin/python'\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/ldap_home/meng.hu/anaconda3/envs/python36/bin/python'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/share/miniconda2/envs/py36/bin/python'\n",
    "from datetime import datetime, timedelta\n",
    "from os.path import basename\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row, SQLContext\n",
    "import pyspark.sql.functions as sqlf\n",
    "import pyspark.sql.types as sqlt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spark init\n",
    "name = 'fdy_spark_basics_demo'\n",
    "port = 30011\n",
    "SPARK_SERVICE = None\n",
    "\n",
    "SPARK_CONF = SparkConf().set('spark.locality.wait', '1000ms') \\\n",
    "    .set('spark.executor.instances', '80') \\\n",
    "    .set('spark.executor.cores', '2') \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", '400') \\\n",
    "    .set('spark.default.parallelism', '400') \\\n",
    "    .set('spark.executor.memory', '5g') \\\n",
    "    .set('spark.ui.port', port) \\\n",
    "    .set('spark.yarn.queue', 'dev') \\\n",
    "    .set('spark.sql.session.timeZone', 'GMT+7') \\\n",
    "    .set('spark.network.timeout', '500') \\\n",
    "    .set('spark.sql.execution.arrow.enable','true') \\\n",
    "    .set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\") \\\n",
    "\n",
    "sc = SparkContext(SPARK_SERVICE, appName=name, conf=SPARK_CONF)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read and write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# csv\n",
    "# from hdfs dir\n",
    "spark_df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .load(\"hdfs:///projects/regds_fdy/fdy_features_logmerchant_feature2020-03-30.csv\")\n",
    "# from local file\n",
    "pandas_df = pd.read_csv('/ldap_home/meng.hu/notebooks/accurary/test_merchant.csv')\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# write to csv\n",
    "# to hadoop dir\n",
    "spark_df.repartition(1) \\\n",
    "    .write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"hdfs:///projects/regds_fdy/demo_test/demo_shipper.csv\")\n",
    "# to local file\n",
    "pandas_df = spark_df.toPandas()\n",
    "pandas_df.to_csv('out_testshipper.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to json or json array\n",
    "time_from = '2020-05-20 00:00:00'\n",
    "time_to ='2020-05-21 00:00:00'\n",
    "REQUEST_LOG_TAB = 'foody.foody_partner_db__order_assign_shipper_batch_processing_log_tab'\n",
    "sql =  '''\n",
    "        select id, city_id, processing_time, grass_date as date, create_time, processing_info, sharding \n",
    "        from {} \n",
    "        where from_unixtime(create_time) between '{}' and '{}'\n",
    "        and city_id = 217\n",
    "       '''.format(REQUEST_LOG_TAB, time_from, time_to)\n",
    "spark_df = spark.sql(sql)\n",
    "spark_df.cache()\n",
    "\n",
    "# to json\n",
    "import json\n",
    "data_info=json.loads(spark_df.collect()[0][5])\n",
    "with open('demo_one_request.json', 'w') as f:\n",
    "    json.dump(data_info, f)\n",
    "    \n",
    "# to json array\n",
    "pandas_df = spark_df.toPandas()\n",
    "\n",
    "# get a list of dict\n",
    "request_list = []       \n",
    "for idx, row in pandas_df.iterrows():\n",
    "    try:\n",
    "        info = json.loads(row['processing_info'])\n",
    "        request = info['ds_request']\n",
    "        if request != None:\n",
    "            request_list.append(request)\n",
    "    except Exception as e:\n",
    "        continue \n",
    "\n",
    "# dump the list to a json array\n",
    "first_item = True\n",
    "with open('demo_json_array.json', 'w') as out:\n",
    "    out.write('[')\n",
    "    for item in res:\n",
    "        if first_item:\n",
    "            out.write(json.dumps(item))\n",
    "            first_item = False\n",
    "        else:\n",
    "            out.write(\",\" + json.dumps(item))            \n",
    "    out.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to parquet files\n",
    "spark_df.write.parquet(hdfs_path,mode='overwrite')\n",
    "# read from parquet files\n",
    "spark_df=spark.read.parquet('hdfs:///projects/regds_fdy/foody_acceptance/shipper_features/shipper_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hive\n",
    "# demo: save hive table by day\n",
    "def create_hive_table_by_day(df, parquet_loc, table_name, grass_date):\n",
    "\n",
    "    \"\"\"\n",
    "    create hive table\n",
    "    \"\"\"\n",
    "    # drop table if it is already exists.\n",
    "    # spark.sql(\"drop table if exists {table_name}\".format(table_name=table_name))\n",
    "\n",
    "    df.createOrReplaceTempView('temp')\n",
    "    columns = ', '.join(spark.sql('describe temp').rdd.map(lambda row: row[0] + ' ' + row[1]).collect())\n",
    "    sql = '''\n",
    "        create external table if not exists {table_name}\n",
    "        (\n",
    "            {columns}\n",
    "        )\n",
    "        partitioned by (grass_date string)\n",
    "        stored as PARQUET\n",
    "        location '{pqfile}'\n",
    "    '''.format(table_name=table_name, columns=columns, pqfile=parquet_loc)\n",
    "    spark.sql(sql)\n",
    "\n",
    "    # drop partitions\n",
    "    sql = '''\n",
    "    ALTER TABLE {table_name} DROP IF EXISTS \n",
    "    PARTITION  (grass_date='{grass_date}')\n",
    "    '''.format(grass_date=grass_date, table_name=table_name)\n",
    "    spark.sql(sql)\n",
    "\n",
    "    # add partition\n",
    "    pqfile_path = parquet_loc + \"/grass_date={}\".format(grass_date)\n",
    "\n",
    "    sql = '''\n",
    "    ALTER TABLE {table_name} ADD IF NOT EXISTS\n",
    "          PARTITION  (grass_date='{grass_date}')\n",
    "          LOCATION '{pqfile}'\n",
    "    '''.format(grass_date=grass_date, table_name=table_name, pqfile=pqfile_path)\n",
    "    spark.sql(sql)\n",
    "    print(\"Insert new data to Table %s with partition (grass_date: {})\".format(grass_date))\n",
    "    \n",
    "request_log_df.write.mode(\"overwrite\").parquet(cur_hdfs_path + partition)\n",
    "create_hive_table_by_day(request_log_df, cur_hdfs_path, table_name, time_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Common RDD Operators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "rdd = sc.parallelize(list(range(0, 10)), 3)\n",
    "def map_func(x):\n",
    "    return 2*x\n",
    "rdd1 = rdd.map(map_func)\n",
    "rdd1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapPartitions\n",
    "def mappartition_func(part):\n",
    "    for x in part:\n",
    "        res = x*2\n",
    "        yield res\n",
    "        \n",
    "rdd2 = rdd.mapPartitions(mappartition_func)\n",
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (0, 4),\n",
       " (1, 6),\n",
       " (1, 8),\n",
       " (1, 10),\n",
       " (2, 12),\n",
       " (2, 14),\n",
       " (2, 16),\n",
       " (2, 18)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapPartitionsWithIndex\n",
    "def mappartitionswithindex_func(partitionindex, part):\n",
    "    for x in part:\n",
    "        res = x*2\n",
    "        yield (partitionindex,res)\n",
    "rdd3 = rdd.mapPartitionsWithIndex(mappartitionswithindex_func)\n",
    "rdd3.take(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 2, 3, 3, 4, 4]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 3)\n",
    "rdd4 = rdd.flatMap(lambda x: (x, x))\n",
    "rdd4.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 4]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glom\n",
    "res = rdd4.glom()\n",
    "def glom_map(x):\n",
    "    return len(x)\n",
    "rdd5 = res.map(glom_map)\n",
    "rdd5.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 3)\n",
    "def groupby_func(x):\n",
    "    return x % 2 == 0\n",
    "grouped = rdd.groupBy(groupby_func) # grouped.getNumPartitions() = 400\n",
    "grouped_list = grouped.map(lambda x: (list(x[1])))\n",
    "grouped_list.map(lambda x: max(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct\n",
    "rdd = sc.parallelize([5, 1, 2, 3, 4, 4, 5], 3)\n",
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoalescedRDD[150] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce\n",
    "rdd = sc.parallelize(list(range(0, 9)), 3)\n",
    "# reduce 3 partitions to 2 by combining last two together\n",
    "rdd.coalesce(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repartitionAndSortWithinPartitions, now change partition number to 200\n",
    "rdd.repartitionAndSortWithinPartitions(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  1| Alice| 18|\n",
      "|  2|  Andy| 19|\n",
      "|  3|   Bob| 17|\n",
      "|  4|Justin| 21|\n",
      "|  5| Cindy| 20|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create df by rdd\n",
    "rdd = sc.parallelize([(1,'Alice', 18),(2,'Andy', 19),(3,'Bob', 17),(4,'Justin', 21),(5,'Cindy', 20)])\n",
    "schema = sqlt.StructType([\n",
    "    sqlt.StructField(\"id\", sqlt.IntegerType(), True),\n",
    "    sqlt.StructField(\"name\", sqlt.StringType(), True),\n",
    "    sqlt.StructField(\"age\", sqlt.IntegerType(), True)\n",
    "])\n",
    "spark_df = rdd.toDF(schema=schema)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Common DF Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: decimal(20,0), city_id: bigint, processing_time: bigint, date: string, create_time: bigint, processing_info: string, sharding: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the df\n",
    "time_from = '2020-05-30 00:00:00'\n",
    "time_to ='2020-06-02 00:00:00'\n",
    "REQUEST_LOG_TAB = 'foody.foody_partner_db__order_assign_shipper_batch_processing_log_tab'\n",
    "sql =  '''\n",
    "        select id, city_id, processing_time, grass_date as date, create_time, processing_info, sharding \n",
    "        from {} \n",
    "        where from_unixtime(create_time) between '{}' and '{}'\n",
    "        and city_id = 217\n",
    "       '''.format(REQUEST_LOG_TAB, time_from, time_to)\n",
    "spark_df = spark.sql(sql)\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_df.count()\n",
    "spark_df.collect()[0]['id'] \n",
    "spark_df.describe('date','create_time').show()\n",
    "spark_df.first() # first row\n",
    "spark_df.head() # first row, spark_df.head(2) # first two rows\n",
    "spark_df.show() # return in df format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_df.columns # return a list of column names\n",
    "spark_df.dtypes # return a list of tuples of (column name, data type)\n",
    "spark_df.printSchema() # print column names and types in tree format\n",
    "spark_df.schema # return column name and type in structType format\n",
    "spark_df.createOrReplaceTempView('demo_df') # create temporary table later can qurey from\n",
    "\n",
    "spark_df.cache() \n",
    "spark_df.unpersist\n",
    "\n",
    "spark_df.groupBy() \n",
    "spark_df.agg()\n",
    "spark_df.filter()\n",
    "spark_df.orderBy()\n",
    "spark_df.sample()\n",
    "\n",
    "spark_df.dropDuplicates()\n",
    "spark_df.na.drop() \n",
    "spark_df.dropna(subset=[])\n",
    "\n",
    "spark_df.select()\n",
    "spark_df.withColumnRenamed()\n",
    "spark_df.withColumn()\n",
    "spark_df.drop()\n",
    "spark_df.explode()\n",
    "\n",
    "spark_df.join()\n",
    "spark_df.union()\n",
    "spark_df.unionByName()\n",
    "spark_df.subtract()\n",
    "spark_df.intersect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'city_id',\n",
       " 'processing_time',\n",
       " 'date',\n",
       " 'create_time',\n",
       " 'processing_info',\n",
       " 'sharding']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|summary|      date|         create_time|\n",
      "+-------+----------+--------------------+\n",
      "|  count|     93641|               93641|\n",
      "|   mean|      null|1.5906830754114437E9|\n",
      "| stddev|      null|   50898.43077166099|\n",
      "|    min|2020-05-28|          1590598800|\n",
      "|    max|2020-05-29|          1590771599|\n",
      "+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.describe('date','create_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: decimal(20,0) (nullable = true)\n",
      " |-- city_id: long (nullable = true)\n",
      " |-- processing_time: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- create_time: long (nullable = true)\n",
      " |-- processing_info: string (nullable = true)\n",
      " |-- sharding: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,DecimalType(20,0),true),StructField(city_id,LongType,true),StructField(processing_time,LongType,true),StructField(date,StringType,true),StructField(create_time,LongType,true),StructField(processing_info,StringType,true),StructField(sharding,StringType,true)))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      date|avg(processing_time)|\n",
      "+----------+--------------------+\n",
      "|2020-05-28|    738.388597067192|\n",
      "|2020-05-29|   709.9667785864737|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.createOrReplaceTempView('demo_df') # create temporary table later can qurey from\n",
    "spark.sql('select date, avg(processing_time) from demo_df where city_id = 217 group by date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      date|min(processing_time)|max(processing_time)|\n",
      "+----------+--------------------+--------------------+\n",
      "|2020-05-28|                  64|               13473|\n",
      "|2020-05-29|                  76|               51463|\n",
      "+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark_df.agg(sqlf.min('processing_time'), sqlf.max('date')).show()\n",
    "# avg, max, min, sum, count\n",
    "spark_df.groupBy('date').agg(sqlf.min('processing_time'), sqlf.max('processing_time')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duplicate, distinct values and fill na "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------------+----+-----------+---------------+--------+\n",
      "|   id|city_id|processing_time|date|create_time|processing_info|sharding|\n",
      "+-----+-------+---------------+----+-----------+---------------+--------+\n",
      "|93641|      1|           3551|   2|      93603|          93641|       1|\n",
      "+-----+-------+---------------+----+-----------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates based on columns\n",
    "spark_df.dropDuplicates(['city_id','id'])\n",
    "# count of distinct values in each column\n",
    "spark_df.agg(*(sqlf.countDistinct(sqlf.col(c)).alias(c) for c in spark_df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------------+------------+-------------------+-----------------------+----------------+\n",
      "|id_missing|city_id_missing|processing_time_missing|date_missing|create_time_missing|processing_info_missing|sharding_missing|\n",
      "+----------+---------------+-----------------------+------------+-------------------+-----------------------+----------------+\n",
      "|       0.0|            0.0|                    0.0|         0.0|                0.0|                    0.0|             0.0|\n",
      "+----------+---------------+-----------------------+------------+-------------------+-----------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# percentage of missing value in each column\n",
    "spark_df.agg(*[(1-(sqlf.count(c) /sqlf.count('*'))).alias(c+'_missing') for c in spark_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "spark_df.filter((spark_df['city_id']==217) & (spark_df['date']=='2020-05-28'))\n",
    "# orderBy\n",
    "spark_df.orderBy(['processing_time','date'], ascending=[True, False]).show()\n",
    "# sample\n",
    "tmp = spark_df.sample(False, 0.2, 42) # (withReplacement, fraction, random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop na\n",
    "spark_df.na.drop() # drop all rows that contain na\n",
    "spark_df.dropna(subset=['id', 'city_id']) # drop rows based on na from subset of columns\n",
    "\n",
    "# fill na\n",
    "# fill na with 0 or any other value\n",
    "spark_df = spark_df.fillna(0, subset=['city_id', 'create_time'])\n",
    "\n",
    "# fill na with column mean or median\n",
    "def fillna_with_mean(df, include=set()): # which columns to include\n",
    "    mean_dict = dict()\n",
    "    for c in include:\n",
    "        mean_dict[c] = df.select(sqlf.mean(df[str(c)])).collect()[0][0]\n",
    "    return df.na.fill(mean_dict)\n",
    "\n",
    "\n",
    "def fillna_with_median(df, include=set()):\n",
    "    median_dict = dict()\n",
    "    for c in include:\n",
    "        median_dict[c] = df.stat.approxQuantile(c, [0.5], 0.001)[0]\n",
    "    return df.na.fill(median_dict)\n",
    "\n",
    "\n",
    "def fillna_with_mean_exclude(df, exclude=set()): # which columns to exclude\n",
    "    mean_dict = dict()\n",
    "    for c in df.columns:\n",
    "        if c not in exclude:\n",
    "            mean_dict[c] = df.select(sqlf.mean(df[str(c)])).collect()[0][0]\n",
    "    return df.na.fill(mean_dict)\n",
    "\n",
    "\n",
    "def fillna_with_median_exclude(df, exclude=set()):\n",
    "    median_dict = dict()\n",
    "    for c in df.columns:\n",
    "        if c not in exclude:\n",
    "            median_dict[c] = df.stat.approxQuantile(c, [0.5], 0.001)[0]\n",
    "    return df.na.fill(median_dict)\n",
    "\n",
    "spark_df = fillna_with_median(spark_df, ['city_id','processing_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: decimal(20,0), city_id: bigint, processing_time: bigint, date: string, create_time: bigint, processing_info: string, sharding: string]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select (select columns)\n",
    "spark_df.select('date','processing_time'+1)\n",
    "# withColumnRenamed (rename a column)\n",
    "spark_df.withColumnRenamed('date','date_new')\n",
    "# withColumn, can add column (can write udf to add column), can used to change column type\n",
    "# my udf\n",
    "import json\n",
    "def extract_num_order(data):\n",
    "    num = 0\n",
    "    data_dict = json.loads(str(data))\n",
    "    if type(data_dict) == dict:\n",
    "        if data_dict[\"ds_request\"] != None:\n",
    "            num = len(data_dict[\"ds_request\"][\"order_shippers\"])\n",
    "    return num\n",
    "\n",
    "# common return type: LongType(), StringType(), IntegerType(), FloatType() \n",
    "# careful: default is String, if don't pass the second parameter\n",
    "extract_num_order_udf = sqlf.udf(extract_num_order, sqlt.IntegerType()) # register udf\n",
    "\n",
    "spark_df = spark_df.withColumn(\"num_order\", extract_num_order_udf(spark_df['processing_info']))\n",
    "# can also used to change column type (demo: change to StringType)\n",
    "spark_df = spark_df.withColumn('processing_time', sqlf.col('processing_time').cast(sqlt.StringType()))\n",
    "# spark_df = spark_df.withColumn('complete_datetime', sqlf.to_timestamp(sqlf.col('create_time')))\n",
    "\n",
    "# drop (drop columns)\n",
    "spark_df.drop('city_id','date')\n",
    "\n",
    "# join/subtract/union of dataframes\n",
    "# join: the join key of two df cannot have same names (rename first)\n",
    "# join type: inner(default), outer, left, right\n",
    "joined_df = df1.join(df2, (df1.res_id==df2.res_id1) & (df1.cid==df2.cid2), \"left\")\n",
    "# union, unionByName\n",
    "tmp = df1.union(df2) # just add rows togeter, doesn't consider column order\n",
    "tmp = df1.unionByName(df2) # use when the order of columns of DFs are different\n",
    "# substract\n",
    "tmp = df1.subtract(df2) # return rows in df1 but not in df2\n",
    "# intersect\n",
    "tmp = df1.intersect(df2) # return rows in both df1 and df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pyarrow and pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.261234998703003\n"
     ]
    }
   ],
   "source": [
    "# case 1 udf\n",
    "import time\n",
    "def which_day(data):\n",
    "    if data == '2020-05-28':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "which_day_udf = sqlf.udf(which_day, sqlt.IntegerType())\n",
    "\n",
    "start = time.time()\n",
    "spark_df = spark_df.withColumn(\"demo1\", which_day_udf(spark_df['date']))\n",
    "spark_df.count()\n",
    "end = time.time()\n",
    "print(end-start)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7254760265350342\n"
     ]
    }
   ],
   "source": [
    "# case 1 pandas_udf\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "def which_day_pd(data):\n",
    "    # the difference with normal udf is that you need to return a pd.series\n",
    "    return pd.Series([1 if a == '2020-05-28' else 2 for a in data])\n",
    "which_day_pd_udf = pandas_udf(which_day_pd, returnType=sqlt.IntegerType()) \n",
    "\n",
    "start = time.time()\n",
    "spark_df = spark_df.withColumn(\"demo2\", which_day_pd_udf(spark_df['date']))\n",
    "spark_df.count()\n",
    "end = time.time()\n",
    "print(end-start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.077404737472534\n"
     ]
    }
   ],
   "source": [
    "# case2 udf\n",
    "import json\n",
    "def extract_num_order(data):\n",
    "    num = 0\n",
    "    data_dict = json.loads(str(data))\n",
    "    if type(data_dict) == dict:\n",
    "        if data_dict[\"ds_request\"] != None:\n",
    "            num = len(data_dict[\"ds_request\"][\"order_shippers\"])\n",
    "    return num\n",
    "extract_num_order_udf = sqlf.udf(extract_num_order, sqlt.IntegerType())\n",
    "\n",
    "start = time.time()\n",
    "spark_df = spark_df.withColumn(\"demo4\", extract_num_order_udf(spark_df['processing_info']))\n",
    "spark_df.count()\n",
    "end = time.time()\n",
    "print(end-start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.39384126663208\n"
     ]
    }
   ],
   "source": [
    "# case 2 pandas_udf\n",
    "def extract_num_order_pd(data):\n",
    "    res = []\n",
    "    for a in data:\n",
    "        num = 0\n",
    "        data_dict = json.loads(str(a))\n",
    "        if type(data_dict) == dict:\n",
    "            if data_dict[\"ds_request\"] != None:\n",
    "                num = len(data_dict[\"ds_request\"][\"order_shippers\"])\n",
    "        res.append(num)\n",
    "    return pd.Series(res)\n",
    "extract_num_order_pd_udf = pandas_udf(extract_num_order_pd, returnType=sqlt.IntegerType()) \n",
    "\n",
    "start = time.time()\n",
    "spark_df = spark_df.withColumn(\"demo5\", extract_num_order_pd_udf(spark_df['processing_info']))\n",
    "spark_df.count()\n",
    "end = time.time()\n",
    "print(end-start) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
